# -*- coding: utf-8 -*-
"""helper_tensorflow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tZNxv509lcHgk2RQ91gMkwlvX8_PWOWr
"""

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import datetime

## Callbacks ##
# LearningRateScheduler
def callbacks_learning_rate(epoch):
  """
  Finding ideal learing rate by change learning rate.
  Need to plot the learning rate vs loss plot to find the best value
  Example: plot_lr_loss(epochs=10)
  -> history_8 = model_8.fit(train_data, train_labels, epochs=10, validation_data=(test_data, test_labels),
                        callbacks=[callbacks_learning_rate(epoch=10)])
  """
  callbacks_lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10**(epoch/20))
  return callbacks_lr_scheduler


# Early Stopping
def callbacks_early_stopping(monitor: str, patience: int, verbose=0):
  """
  Early stop if the model is not improve.
  Example: 
  - For classification model: callbacks_early_stopping(monitor="accuracy", patience=20, verbose=1)
  - For regression model: callbacks_early_stopping(monitor="mae", patience=100, verbose=1)
  -> history_8 = model_8.fit(train_data, train_labels, epochs=10, validation_data=(test_data, test_labels),
                        callbacks=[callbacks_early_stopping(monitor="accuracy", patience=5)])
  """
  callbacks_early_stopping = tf.keras.callbacks.EarlyStopping(monitor=monitor, patience=patience, verbose=verbose, restore_best_weights=True)
  return callbacks_early_stopping

# Tensorboard callback
def callback_create_tensorboard(dir_name: str, experiment_name: str):
  """
  Example: callbacks=[callback_create_tensorboard(dir_name="tensorflow_hub",
                                                  experiment_name="efficientnet")])
  """
  log_dir = dir_name + "/" + experiment_name + "/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
  tensorboard_callback = tf.keras.callbacks.TensorBoard(
      log_dir=log_dir
  )
  print(f"Saving TensorBoard log files to: {log_dir}")
  return tensorboard_callback


# ================================ #


## Plot graph ##
# Plot loss vs learning rate curve
def plot_lr_loss(history, figsize=(10,7)):
  """
  * Must use with callbacks_learning_rate() *
  Plot the learning rate versus the loss by getting history from the model training.
  """
  lrs = 1e-4 * (10 ** (np.arange(100)/20))
  plt.figure(figsize=figsize)
  plt.semilogx(lrs, history.history["loss"]) # we want the x-axis (learning rate) to be log scale
  plt.xlabel("Learning Rate")
  plt.ylabel("Loss")
  plt.title("Learning rate vs. loss")

# Plot the validation and training data separately
def plot_loss_curves(history,figsize=(7,7)):
  """
  Returns separate loss curves for training and validation metrics.
  """ 
  plt.figure(figsize=figsize)
  loss = history.history['loss']
  val_loss = history.history['val_loss']

  accuracy = history.history['accuracy']
  val_accuracy = history.history['val_accuracy']

  epochs = range(len(history.history['loss']))

  # Plot loss
  plt.plot(epochs, loss, label='training_loss')
  plt.plot(epochs, val_loss, label='val_loss')
  plt.title('Loss')
  plt.xlabel('Epochs')
  plt.legend()

  # Plot accuracy
  plt.figure()
  plt.plot(epochs, accuracy, label='training_accuracy')
  plt.plot(epochs, val_accuracy, label='val_accuracy')
  plt.title('Accuracy')
  plt.xlabel('Epochs')
  plt.legend();

